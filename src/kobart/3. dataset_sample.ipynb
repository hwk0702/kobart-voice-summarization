{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "downtown-rough",
   "metadata": {},
   "source": [
    "# Dataset sample\n",
    "\n",
    "DataLoader에 들어갈 Dataset 클래스 데이터를 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "actual-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SummaryDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "otherwise-melissa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "Load 10000 dev sample.\n"
     ]
    }
   ],
   "source": [
    "dset = SummaryDataset(root_path='./data', mode='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "charged-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "behind-means",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([16058, 14064, 13109, 10746, 13109, 19995,   300,   306,   304, 14759,\n",
       "           300,   306,   304, 15092, 23608, 14864, 29038, 20077, 14196, 15506,\n",
       "         22655, 18553, 12007, 14615, 14363, 25740, 21158, 14437, 15682, 22002,\n",
       "         13590, 14522, 15506, 22655, 18553, 14063,   274, 20165,   282,   243,\n",
       "         14572, 21292,   274,   282,   239, 13109, 16603,  1700, 13110, 11440,\n",
       "         19632, 14412, 14383, 14235, 15005, 16368, 22002, 14343, 14322, 18553,\n",
       "         14063,   275, 22995,   274, 15516,   277,   267, 14676,   278,   275,\n",
       "           239, 10214, 11725, 10343,   240, 16302, 14064, 13109, 10746, 13109,\n",
       "         12024, 22721, 14464,  1700, 13360, 13090, 10277, 11418, 21352, 15166,\n",
       "         14583, 14032, 14857, 18461, 22002, 19109, 15646, 14282, 14193, 10298,\n",
       "          9049, 19754, 14025, 14591, 14064, 13109, 10746, 13109,  8981, 16180,\n",
       "         22845, 15210, 19211, 14141, 16625, 15487, 19539, 20938, 12007, 14232,\n",
       "         14196, 15506, 22655, 18553, 12024, 21004, 16395, 14063,   279, 16968,\n",
       "           306,   320, 15073, 16968,   306,   320,   239, 13571, 13109, 13571,\n",
       "         13109, 19632, 23832, 16411, 15880, 12123, 22421, 14623, 13607, 25740,\n",
       "         11786, 14152, 14608, 14773, 14614, 10338, 20512, 21006, 15059, 16899,\n",
       "         14299, 14064, 13109, 10746, 18298, 15053, 18491, 23832, 15880, 12123,\n",
       "         11786, 14311, 15402, 11011, 10476, 25422, 15742, 14030, 14148, 13019,\n",
       "          9036, 10338, 19954,   243, 22069, 14693, 12049, 13607, 26667, 17460,\n",
       "         11786, 27053, 14058, 15964, 28773, 16112, 14063, 17416, 13360, 14385,\n",
       "         10277, 11418, 22470, 26283,   271, 15054,   268,   268,   277,   244,\n",
       "           266,   281,   284,   282,   271, 19632, 14174, 14790, 20914, 14064,\n",
       "         13109, 10746, 13109, 12024, 20927, 15668, 17591, 20084, 18998,  9698,\n",
       "         14130, 21891, 16411, 15880, 12123, 16899, 14147, 21004,  9103, 14063,\n",
       "           279, 16968,   306,   320, 15073, 16968,   306,   320,   238, 14105,\n",
       "         10667, 11863, 14270, 17014, 14509, 23499, 11280, 14082, 28170, 10012,\n",
       "         12024, 28452, 14623, 15817, 27026, 11786, 14152, 24784, 19728, 14276,\n",
       "         20029, 14064, 13109, 10746, 13109, 12024, 21004,  9103, 14063,   279,\n",
       "         16968,   306,   320, 15073, 16968,   306,   320, 15731, 29724, 13109,\n",
       "         13590, 28846, 11786, 14063,   279, 16968,   306,   320, 15073,   304,\n",
       "           306,   298,   320, 16726, 23499, 11280, 14082,  1700, 13757, 12034,\n",
       "         29141, 16395, 16247, 14028, 17441, 14484, 15880, 12123, 10443, 14232,\n",
       "         18491, 14063,   275,   304,   306,   300,  1700, 16805,   243, 18785,\n",
       "         14889,   238,   315, 25674,   304,   306,   300,  1700, 16805,   245,\n",
       "         14676,   316,   317,  1700, 16805,   243, 18785, 14889,   238,   315,\n",
       "         20676, 15195,   315,  1700, 16805,   245, 16996,   300,   306,   304,\n",
       "         14759,   300,   306,   304,   243, 14943,   238,   308, 18482,   310,\n",
       "         15073, 16968,   306,   320, 16726, 14039, 16602, 18845, 10000, 14347,\n",
       "         10955, 16911, 22871, 14159, 28473, 15667, 14064, 13109, 10746, 13109,\n",
       "         12024, 21742, 20914, 14063, 13360, 13090, 10277, 11418, 15455, 14480,\n",
       "         18474, 25321, 14498, 16247, 14064, 13109, 10746, 18298, 14864, 29038,\n",
       "         20077, 14670, 19831, 14196, 15506, 22655, 18553, 14063,   275, 22995,\n",
       "           274, 15516,   277,   267, 14676,   278,   275, 14989, 21173, 22002,\n",
       "         21158, 14322, 18553, 12005, 21004,  9103, 14063,   279, 16968,   306,\n",
       "           320, 15073, 16968,   306,   320, 15127, 16464, 14281, 14086,  9103,\n",
       "         12034, 23627, 15568, 14297, 28505, 14064, 13109, 10746, 13109, 16718,\n",
       "         14180, 15266, 15166, 14583, 14032, 15964, 16437, 14064, 13109, 10746,\n",
       "         18298, 14864, 16069, 29241, 10443, 18285, 18510, 22315, 20147, 21158,\n",
       "             1,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3]),\n",
       " 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'labels': tensor([14141, 16625, 23832, 15880, 12123, 22421, 14232, 17239, 13360, 14385,\n",
       "         10277, 11418, 12024, 21352, 18845, 10582, 14608, 14773, 20512, 10479,\n",
       "         16058, 14064, 13109, 10746, 13109,  8981, 14864, 29038, 20077, 29724,\n",
       "         13109, 13590, 28846, 12024, 23499, 11280, 14082,  1700, 13757, 12007,\n",
       "         15687, 14063, 15073, 16968,   306,   320, 15073, 16968,   306,   320,\n",
       "           239, 13571, 13109, 13571, 13109, 19632, 14524, 23443, 22655, 18553,\n",
       "         12007, 14615, 14363, 25740, 21158,     1,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-wiring",
   "metadata": {},
   "source": [
    "```-100```은 CrossEntropy loss의 기본적으로 설정되어 있는 ignore_index입니다. <br>\n",
    "Loss를 계산할 때 해당 위치에서의 계산은 무시하는 역할을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-throat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
